{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image classification with CNNs\n",
    "================\n",
    "\n",
    "The goal of this exercise is to implement a specific CNN architecture with PyTorch and train it on the CIFAR-10 image classification dataset. We will start by introducing the dataset and then implement a `nn.Module` and a useful `Solver` class. Seperating the model from the actual training has proven itself as a sensible design decision. By the end of this exercise you should have succesfully trained your (possible) first CNN model and have a boilerplate `Solver` class which you can reuse for the next exercise and your future research projects.\n",
    "\n",
    "For an inspiration on how to implement a model or the solver class you can have a look at [these](https://github.com/pytorch/examples) PyTorch examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#from dl4cv.classifiers.classification_cnn import ClassificationCNN\n",
    "from yz.data_utils import get_Cancer_datasets\n",
    "#torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "csv_full_name = '/home/ubuntu/dl4cvproject/data/train.csv'\n",
    "img_folder_full_name = '/home/ubuntu/dl4cvproject/data/train_256'\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cancer Dataset\n",
    "=========\n",
    "\n",
    "Since the focus of this exercise should be neural network models and how to successfully train them, we provide you with preprocessed and prepared datasets. For an even easier management of the train, validation and test data pipelines we provide you with custom `torch.utils.data.Dataset` classes. Use the official [documentation](http://pytorch.org/docs/data.html) to make yourself familiar with the `Dataset` and `DataLoader` classes. Think about how you have to integrate them in your training loop and have a look at the data preprocessing steps in `dl4cv/data_utils.py`.\n",
    "\n",
    "The `num_workers` argument of the `DataLoader` class allows you to preprocess data with multiple threads.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Note</h3>\n",
    "    <p>In this case we generated the `Dataset` classes after we applied all the preprocessing steps. Other datasets or random data augmentation might require an online preprocessing which can be integrated into the `Dataset` classes. See `torchvision.Transform` for examples.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 18545/18577 [00:42<00:00, 441.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming...\n",
      "X.shape:(18577, 256, 256, 3)\n",
      "X_original.shape:(18577, 256, 256)\n",
      "mean:125.83685163263934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/18577 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 18577/18577 [00:00<00:00, 1188021.61it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std:63.15855322359238\n",
      "Done transforming...\n",
      "Getting labels\n",
      "submasking...\n",
      "num_training:16000\n",
      "OK...\n",
      "Train size: 16000\n",
      "Val size: 1250\n",
      "Test size: 1327\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data. The preprocessing includes\n",
    "# channel swapping, normalization and train-val-test splitting.\n",
    "# Loading the datasets might take a while.\n",
    "\n",
    "train_data, val_data, test_data, train_label_list = get_Cancer_datasets(csv_full_name=csv_full_name,img_folder_full_name=img_folder_full_name)\n",
    "#train_data, val_data, test_data, train_label_list = get_Cancer_datasets(csv_full_name=csv_full_name,img_folder_full_name=img_folder_full_name,\n",
    "#                                                                                  num_training=50, num_validation=20, num_test=30)\n",
    "# X = get_Cancer_datasets(csv_full_name=csv_full_name,img_folder_full_name=img_folder_full_name, num_training=800, num_validation=100, num_test=100)\n",
    "print(\"Train size: %i\" % len(train_data))\n",
    "print(\"Val size: %i\" % len(val_data))\n",
    "print(\"Test size: %i\" % len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'yz.data_utils.CancerData'> <class 'yz.data_utils.CancerData'> <class 'yz.data_utils.CancerData'>\n",
      "\n",
      "label_type: <class 'numpy.int64'>\n",
      "img_type: <class 'torch.FloatTensor'>\n",
      "img_shape:torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "#print(len(train_data))\n",
    "print(type(train_data), type(val_data), type(test_data))\n",
    "print()\n",
    "for i in range(1):\n",
    "    inputs, labels = train_data[i]\n",
    "    print('label_type: {}'.format(type(labels)))\n",
    "\n",
    "img, lb = train_data[i]\n",
    "print('img_type: {}'.format(type(img)))\n",
    "print('img_shape:{}'.format(img.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-4-a0eb1ccc0802>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-a0eb1ccc0802>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    cnt[num] += 1\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|█████████▉| 18545/18577 [01:00<00:00, 308.82it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "label_set = set()\n",
    "cnt = [0] * 14\n",
    "for i in tqdm(range(len(train_data))):\n",
    "    _, labels = train_data[i]\n",
    "    for num in range(len(cnt)):\n",
    "        if labels == num:70\n",
    "            cnt[num] += 1\n",
    "    label_set.add(labels)\n",
    "print(label_set)\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find a sample with certain index\n",
    "\n",
    "class_label = 13\n",
    "for i in range(len(train_data)):\n",
    "    _, labels = train_data[i]\n",
    "    if labels == class_label:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Examples\n",
    "------------------\n",
    "\n",
    "To make yourself familiar with the dataset we visualize some examples. We show a few examples from each class. Note that we have to revert (transposition and mean subtraction) some preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "index = 10009\n",
    "img, _ = train_data[index]\n",
    "print(torch.sum(img > 0))\n",
    "print(img)\n",
    "to_plot = img.cpu().numpy() #+ mean_image\n",
    "print(to_plot.shape)\n",
    "plt.imshow(to_plot[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture and Forward Pass \n",
    "\n",
    "After you understood the core concepts of PyTorch and have a rough idea on how to implement your own model, complete the initialization and forward methods of the `ClassificationCNN` in the `dl4cv/classifiers/classification_cnn.py` file. Note that we do not have to implement a backward pass since this is automatically handled by the `autograd` package.\n",
    "\n",
    "Use the cell below to check your results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation with the Solver\n",
    "We train and validate our previously generated model with a seperate `Solver` class defined in `dl4cv/solver.py`. Complete the `.train()` method and try to come up with an efficient iteration scheme as well as an informative training logger.\n",
    "\n",
    "Use the cells below to test your solver. A nice trick is to train your model with just a few training samples. You should be able to overfit small datasets, which will result in very high training accuracy and comparatively low validation accuracy.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Note</h3>\n",
    "    <p>As seen below, the design of our `Solver` class is indepdenent of the particular model or data pipeline. This facilitates the reuse of the class and its modular structure allows the training of different models.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the loss, training accuracy, and validation accuracy should show clear overfitting:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Network\n",
    "Now train your model with the full dataset. By training a `ThreeLayerCNN` model for one epoch, you should already achieve greater than 40% accuracy on the validation set. If your training is painfully slow check if you did not forget to call the `nn.Module.cuda()` method.\n",
    "\n",
    "For the overfitting example we provided you with a set of hyperparamters (`hidden_dim`, `lr`, `weight_decay`, ...). You can start with the same parameter values but in order to maximize your accuracy you should try to train multiple models with different sets of hyperparamters. This process is called hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from yz.classifiers.classification_cnn import ClassificationCNN\n",
    "from yz.classifiers.transferred_alexnet import alexnet\n",
    "from yz.solver import Solver\n",
    "from yz.data_utils import get_balanced_weights\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "weights = get_balanced_weights(train_label_list, 14)\n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights))\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=50, shuffle=False, sampler=sampler, num_workers=8)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=50, shuffle=False, num_workers=8)\n",
    "\n",
    "\n",
    "model = models.resnet101(pretrained=True)\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features, 14)\n",
    "\n",
    "\"\"\"\n",
    "VGG16\n",
    "\n",
    "model = models.vgg16(pretrained=True)\n",
    "model.classifier  = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 14),\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "#list(model.classifier.children())[:-1] = nn.Linear(4096, 14)  \n",
    "if torch.cuda.is_available():\n",
    "    print('Cuda available')\n",
    "    model.cuda()\n",
    "\n",
    "\n",
    "solver = Solver()\n",
    "solver.train(model, train_loader, val_loader, log_nth=1, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=50, shuffle=False, sampler=sampler, num_workers=8, drop_last=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=50, shuffle=False, num_workers=8)\n",
    "solver = Solver()\n",
    "solver.train(model, train_loader, val_loader, log_nth=2, num_epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test your Model\n",
    "Run your best model on the test set. You should easily achieve a score above 10% (random guessing for a classification task with 10 classes) accuracy on the given test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=30, shuffle=False, num_workers=8)\n",
    "\n",
    "scores = []\n",
    "for inputs, target in tqdm(test_loader):\n",
    "    #print(type(target))\n",
    "    inputs, targets = Variable(inputs), Variable(target)\n",
    "    if torch.cuda.is_available:\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    scores.extend((preds == targets).data.cpu().numpy())\n",
    "    \n",
    "print('Test set accuracy: %f' % np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get final test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v = csv_test['detected'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from yz.data_utils import get_Cancer_datasets\n",
    "csv_full_name = '/home/ubuntu/dl4cvproject/data/test.csv'\n",
    "img_folder_full_name = '/home/ubuntu/dl4cvproject/data/test_256'\n",
    "test_X, csv_test = get_Cancer_datasets(csv_full_name=csv_full_name,img_folder_full_name=img_folder_full_name, mode='upload')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = 20\n",
    "img = test_X[index]\n",
    "print(torch.sum(img > 0))\n",
    "print(img)\n",
    "print(type(img))\n",
    "to_plot = img.cpu().numpy() #+ mean_image\n",
    "print(to_plot.shape)\n",
    "plt.imshow(to_plot[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(type(csv_test))\n",
    "print(len(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del csv_test['age']\n",
    "except KeyError as e:\n",
    "    print(e)\n",
    "try:\n",
    "    del csv_test['gender']\n",
    "except KeyError as e:\n",
    "    print(e)\n",
    "try:\n",
    "    del csv_test['view_position']\n",
    "except KeyError as e:\n",
    "    print(e)\n",
    "try:\n",
    "    del csv_test['image_name']\n",
    "except KeyError as e:\n",
    "    print(e)\n",
    "try:\n",
    "    del csv_test['detected']\n",
    "except KeyError as e:\n",
    "    print(e)\n",
    "\n",
    "print(list(csv_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = test_X[0]\n",
    "print(inputs.unsqueeze(0).size())\n",
    "test = torch.cat((inputs.unsqueeze(0), inputs.unsqueeze(0)), 0)\n",
    "print(test.size())\n",
    "if torch.cuda.is_available:\n",
    "    inputs = inputs.cuda()\n",
    "    test.cuda()\n",
    "outputs = model(Variable(test).cuda())\n",
    "_, preds = torch.max(outputs, 1)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and Submission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del csv_test['age']\n",
    "except KeyError as e:\n",
    "    print(e)\n",
    "try:\n",
    "    del csv_test['gender']\n",
    "except KeyError as e:\n",
    "    print(e)\n",
    "try:\n",
    "    del csv_test['view_position']\n",
    "except KeyError as e:\n",
    "    print(e)\n",
    "try:\n",
    "    del csv_test['image_name']\n",
    "except KeyError as e:\n",
    "    print(e)\n",
    "try:\n",
    "    del csv_test['detected']\n",
    "except KeyError as e:\n",
    "    print(e)\n",
    "\n",
    "print(list(csv_test))\n",
    "\n",
    "import pandas as pd\n",
    "index = 0\n",
    "jump = 30\n",
    "detected = []\n",
    "pred_set = set()\n",
    "debug_length = 10\n",
    "for i in tqdm(range(len(test_X))):\n",
    "#for i in tqdm(range(debug_length)):\n",
    "    #start = index\n",
    "    #end = index + jump\n",
    "    #if end >= (test_X.size()[0]) :\n",
    "    #    end = test_X.size()[0]\n",
    "    tmp_pred_list = [0] * 14\n",
    "    for trial in range(7):\n",
    "        inputs = test_X[i]\n",
    "        inputs = Variable(inputs.unsqueeze(0))\n",
    "        if torch.cuda.is_available:\n",
    "            inputs = inputs.cuda()\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        int_label = preds.data.cpu().numpy().tolist()[0]\n",
    "        tmp_pred_list[int_label] += 1\n",
    "    \n",
    "    pred = tmp_pred_list.index(max(tmp_pred_list))\n",
    "    #print(tmp_pred_list, pred)\n",
    "    str_pred = 'class_' + str(pred + 1)\n",
    "    detected.append(str_pred)\n",
    "    ###\n",
    "    #int_list_preds = preds.data.cpu().numpy().tolist()\n",
    "    #for pred_num in int_list_preds:\n",
    "    #    pred_set.add(pred_num + 1)\n",
    "    #str_list_preds = [('class_' + str(pred_num + 1)) for pred_num in int_list_preds]\n",
    "    #detected.extend(str_list_preds)\n",
    "    ####\n",
    "    #if end == test_X.size()[0]:\n",
    "    #    break\n",
    "    #index += jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(pred_set)\n",
    "csv_test['detected'] = pd.Series(detected)\n",
    "csv_test.to_csv('submission.csv', index=False)\n",
    "print(csv_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "model.save(\"models/classification_cnn.model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
